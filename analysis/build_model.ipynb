{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Roach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed Packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Feature Engineering\n",
    "Before any modeling was performed, the necessary data was collected using two distinct platforms. The first data that was collected was Twitter data. This was done using the Twitter Developer API, as well as the `tweepy` module. Tweets containing the word \"bitcoin\" were streamed for several days. This data was written to a `.pkl` file, and saved for later feature engineering.\n",
    "\n",
    "The other data that was collected was the value of a single Bitcoin. During the same interval (plus twenty-four hours after the last Tweet was recorded) that the Twitter data was collected, the value of a Bitcoin was recorded each minute, along with the corresponding time stamp.\n",
    "\n",
    "Once the Twitter and Bitcoin data was recorded, further feature engineering was employed. For each Tweet stored, the corresponding price of Bitcoin at the time the Tweet was made was added as the `inital_price` for the Tweet. Then, for each Tweet, if the price of Bitcoin increased within three hours of the time the Tweet was made, the feature `increase` was assigned a value of `1`. Otherwise, `increase` is assigned the value of `0`.\n",
    "\n",
    "Finally, for each Tweet recorded, the text of that Tweet is cleaned and standardized. This cleaned Tweet is then BERTified, and a vector of length 384 is returned. This vector is stored as the `embedded` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data.\n",
    "data = pd.read_pickle(\"../data/3_23_training_data.pkl\")\n",
    "\n",
    "# Reset the index, convert each embedding to an array.\n",
    "data = data.reset_index(drop=True)\n",
    "data[\"embedding\"] = data[\"embedding\"].apply(lambda x: np.asarray(x))\n",
    "\n",
    "# Remove bad rows.\n",
    "max_stamps = map(lambda x: x - timedelta(hours=3), set(data[\"time\"].tolist()))\n",
    "data = data.loc[data[\"time\"].isin(list(max_stamps)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split (for aggregation).\n",
    "stamps = np.unique(data.time)\n",
    "data.set_index([\"time\"], inplace=True)\n",
    "test_stamps = np.random.choice(stamps, size=int(stamps.shape[0] * .20))\n",
    "test_data = data.loc[test_stamps, :]\n",
    "train_data = data.loc[~data.index.isin(test_stamps), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training data has been read in, the data will be quickly inspected, to show the reader the nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 305282 rows in the DataFrame.\n",
      "There are 157735 records with an increase, and 147547 with a decrease.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate the DataFrame.\n",
    "print(\"There are\", len(data), \"rows in the DataFrame.\")\n",
    "print(\"There are\", len(data.loc[data[\"increase\"] == 1, ]), \"records with an increase, and\", \n",
    "        len(data.loc[data[\"increase\"] == 0, ]), \"with a decrease.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing data.\n",
    "x_train = train_data[\"embedding\"]\n",
    "y_train = train_data[\"increase\"]\n",
    "x_test = test_data[\"embedding\"]\n",
    "y_test = test_data[\"increase\"]\n",
    "\n",
    "# Conver to Tensors.\n",
    "x_train = tf.convert_to_tensor(x_train.to_list())\n",
    "y_train = tf.convert_to_tensor(y_train.to_list())\n",
    "x_test = tf.convert_to_tensor(x_test.to_list())\n",
    "y_test = tf.convert_to_tensor(y_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "7739/7739 [==============================] - 30s 4ms/step - loss: 0.6890 - accuracy: 0.5369 - val_loss: 0.6837 - val_accuracy: 0.5520\n",
      "Epoch 2/25\n",
      "7739/7739 [==============================] - 30s 4ms/step - loss: 0.6817 - accuracy: 0.5535 - val_loss: 0.6840 - val_accuracy: 0.5380\n",
      "Epoch 3/25\n",
      "7739/7739 [==============================] - 29s 4ms/step - loss: 0.6790 - accuracy: 0.5600 - val_loss: 0.6775 - val_accuracy: 0.5653\n",
      "Epoch 4/25\n",
      "7739/7739 [==============================] - 30s 4ms/step - loss: 0.6772 - accuracy: 0.5637 - val_loss: 0.6754 - val_accuracy: 0.5716\n",
      "Epoch 5/25\n",
      "7739/7739 [==============================] - 28s 4ms/step - loss: 0.6758 - accuracy: 0.5666 - val_loss: 0.6777 - val_accuracy: 0.5584\n",
      "Epoch 6/25\n",
      "7739/7739 [==============================] - 29s 4ms/step - loss: 0.6745 - accuracy: 0.5685 - val_loss: 0.6730 - val_accuracy: 0.5721\n",
      "Epoch 7/25\n",
      "7739/7739 [==============================] - 29s 4ms/step - loss: 0.6736 - accuracy: 0.5706 - val_loss: 0.6752 - val_accuracy: 0.5713\n",
      "Epoch 8/25\n",
      "7739/7739 [==============================] - 29s 4ms/step - loss: 0.6729 - accuracy: 0.5721 - val_loss: 0.6749 - val_accuracy: 0.5667\n",
      "Epoch 9/25\n",
      "7739/7739 [==============================] - 30s 4ms/step - loss: 0.6720 - accuracy: 0.5735 - val_loss: 0.6763 - val_accuracy: 0.5687\n",
      "Epoch 10/25\n",
      "7739/7739 [==============================] - 29s 4ms/step - loss: 0.6713 - accuracy: 0.5749 - val_loss: 0.6798 - val_accuracy: 0.5503\n",
      "Epoch 11/25\n",
      "7739/7739 [==============================] - 34s 4ms/step - loss: 0.6707 - accuracy: 0.5767 - val_loss: 0.6747 - val_accuracy: 0.5680\n",
      "Epoch 12/25\n",
      "7739/7739 [==============================] - 32s 4ms/step - loss: 0.6701 - accuracy: 0.5780 - val_loss: 0.6810 - val_accuracy: 0.5528\n",
      "Epoch 13/25\n",
      "7739/7739 [==============================] - 32s 4ms/step - loss: 0.6693 - accuracy: 0.5787 - val_loss: 0.6746 - val_accuracy: 0.5655\n",
      "Epoch 14/25\n",
      "7739/7739 [==============================] - 33s 4ms/step - loss: 0.6690 - accuracy: 0.5790 - val_loss: 0.6818 - val_accuracy: 0.5594\n",
      "Epoch 15/25\n",
      "7739/7739 [==============================] - 33s 4ms/step - loss: 0.6684 - accuracy: 0.5795 - val_loss: 0.6748 - val_accuracy: 0.5708\n",
      "Epoch 16/25\n",
      "7739/7739 [==============================] - 34s 4ms/step - loss: 0.6677 - accuracy: 0.5827 - val_loss: 0.6741 - val_accuracy: 0.5720\n",
      "Epoch 17/25\n",
      "7739/7739 [==============================] - 34s 4ms/step - loss: 0.6674 - accuracy: 0.5828 - val_loss: 0.6746 - val_accuracy: 0.5654\n",
      "Epoch 18/25\n",
      "7739/7739 [==============================] - 34s 4ms/step - loss: 0.6670 - accuracy: 0.5827 - val_loss: 0.6760 - val_accuracy: 0.5641\n",
      "Epoch 19/25\n",
      "7739/7739 [==============================] - 35s 5ms/step - loss: 0.6665 - accuracy: 0.5836 - val_loss: 0.6732 - val_accuracy: 0.5736\n",
      "Epoch 20/25\n",
      "7739/7739 [==============================] - 35s 5ms/step - loss: 0.6659 - accuracy: 0.5852 - val_loss: 0.6742 - val_accuracy: 0.5651\n",
      "Epoch 21/25\n",
      "7739/7739 [==============================] - 33s 4ms/step - loss: 0.6657 - accuracy: 0.5854 - val_loss: 0.6751 - val_accuracy: 0.5679\n",
      "Epoch 22/25\n",
      "7739/7739 [==============================] - 31s 4ms/step - loss: 0.6655 - accuracy: 0.5859 - val_loss: 0.6739 - val_accuracy: 0.5692\n",
      "Epoch 23/25\n",
      "7739/7739 [==============================] - 31s 4ms/step - loss: 0.6651 - accuracy: 0.5856 - val_loss: 0.6777 - val_accuracy: 0.5633\n",
      "Epoch 24/25\n",
      "7739/7739 [==============================] - 32s 4ms/step - loss: 0.6647 - accuracy: 0.5876 - val_loss: 0.6803 - val_accuracy: 0.5550\n",
      "Epoch 25/25\n",
      "7739/7739 [==============================] - 30s 4ms/step - loss: 0.6644 - accuracy: 0.5869 - val_loss: 0.6785 - val_accuracy: 0.5710\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7ff13619bb20>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "input_layer = Input((768,))\n",
    "dense = Dense(128, activation=\"relu\")(input_layer)\n",
    "output = Dense(2, activation=\"softmax\")(dense)  # Output values is the number of classes.\n",
    "rnn_model = Model(input_layer, output)\n",
    "\n",
    "# Compile the model.\n",
    "rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model. MAKE SURE TO CHANGE THIS TO 25 EPOCHS.\n",
    "rnn_model.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models to try:\n",
    "### - SVM\n",
    "### - Naive Bayes\n",
    "### - kNN\n",
    "### - Random Forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to DataFrame.\n",
    "predictions = rnn_model.predict(x_test)\n",
    "predictions = np.array(list(map(lambda x: 0 if x[0] > x[1] else 1, predictions)))\n",
    "test_data[\"prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame.\n",
    "aggregated = pd.DataFrame(test_stamps, columns=[\"time\"])\n",
    "\n",
    "# Get the actual.\n",
    "agg_count = test_data.loc[:, [\"increase\"]].groupby(\"time\").count()\n",
    "agg_sum = test_data.loc[:, [\"increase\", \"prediction\"]].groupby(\"time\").sum()\n",
    "\n",
    "# Change column names.\n",
    "agg_count = agg_count.rename(columns={\"increase\": \"total_count\"})\n",
    "agg_sum = agg_sum.rename(columns={\"increase\": \"actual\", \"prediction\": \"pred_count\"})\n",
    "\n",
    "# Final join.\n",
    "agg = agg_count.join(agg_sum)\n",
    "agg[\"actual\"] = agg[\"actual\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "agg[\"pred_perc\"] = agg[\"pred_count\"] / agg[\"total_count\"]\n",
    "agg = agg[[\"actual\", \"total_count\", \"pred_count\", \"pred_perc\"]]\n",
    "agg.to_csv(\"../data/3_23_agg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e5788945c2802237ac72a84c772d7bec8af624c1002577fbf4f87a46c8d1d03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('scholarly_project': conda)",
   "name": "python391jvsc74a57bd0bec8622bf30164f87f5685ebe98ae91d7fd7e93d67ef4c01423e53cab45c9e9c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}