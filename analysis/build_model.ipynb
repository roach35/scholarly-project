{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Roach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed Packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Feature Engineering\n",
    "Before any modeling was performed, the necessary data was collected using two distinct platforms. The first data that was collected was Twitter data. This was done using the Twitter Developer API, as well as the `tweepy` module. Tweets containing the word \"bitcoin\" were streamed for several days. This data was written to a `.pkl` file, and saved for later feature engineering.\n",
    "\n",
    "The other data that was collected was the value of a single Bitcoin. During the same interval (plus twenty-four hours after the last Tweet was recorded) that the Twitter data was collected, the value of a Bitcoin was recorded each minute, along with the corresponding time stamp.\n",
    "\n",
    "Once the Twitter and Bitcoin data was recorded, further feature engineering was employed. For each Tweet stored, the corresponding price of Bitcoin at the time the Tweet was made was added as the `inital_price` for the Tweet. Then, for each Tweet, if the price of Bitcoin increased within three hours of the time the Tweet was made, the feature `increase` was assigned a value of `1`. Otherwise, `increase` is assigned the value of `0`.\n",
    "\n",
    "Finally, for each Tweet recorded, the text of that Tweet is cleaned and standardized. This cleaned Tweet is then BERTified, and a vector of length 384 is returned. This vector is stored as the `embedded` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data.\n",
    "data = pd.read_pickle(\"../data/3_23_training_data.pkl\")\n",
    "\n",
    "# Reset the index, convert each embedding to an array.\n",
    "data = data.reset_index(drop=True)\n",
    "data[\"embedding\"] = data[\"embedding\"].apply(lambda x: np.asarray(x))\n",
    "\n",
    "# Remove bad rows.\n",
    "max_stamps = map(lambda x: x - timedelta(hours=3), set(data[\"time\"].tolist()))\n",
    "data = data.loc[data[\"time\"].isin(list(max_stamps)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split (for aggregation).\n",
    "stamps = np.unique(data.time)\n",
    "data.set_index([\"time\"], inplace=True)\n",
    "test_stamps = np.random.choice(stamps, size=int(stamps.shape[0] * .20))\n",
    "test_data = data.loc[test_stamps, :]\n",
    "train_data = data.loc[~data.index.isin(test_stamps), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training data has been read in, the data will be quickly inspected, to show the reader the nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 305282 rows in the DataFrame.\n",
      "There are 157735 records with an increase, and 147547 with a decrease.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate the DataFrame.\n",
    "print(\"There are\", len(data), \"rows in the DataFrame.\")\n",
    "print(\"There are\", len(data.loc[data[\"increase\"] == 1, ]), \"records with an increase, and\", \n",
    "        len(data.loc[data[\"increase\"] == 0, ]), \"with a decrease.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing data.\n",
    "x_train = train_data[\"embedding\"]\n",
    "y_train = train_data[\"increase\"]\n",
    "x_test = test_data[\"embedding\"]\n",
    "y_test = test_data[\"increase\"]\n",
    "\n",
    "# Conver to Tensors.\n",
    "x_train = tf.convert_to_tensor(x_train.to_list())\n",
    "y_train = tf.convert_to_tensor(y_train.to_list())\n",
    "x_test = tf.convert_to_tensor(x_test.to_list())\n",
    "y_test = tf.convert_to_tensor(y_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "7803/7803 [==============================] - 41s 5ms/step - loss: 0.6878 - accuracy: 0.5391 - val_loss: 0.6824 - val_accuracy: 0.5522\n",
      "Epoch 2/15\n",
      "7803/7803 [==============================] - 40s 5ms/step - loss: 0.6810 - accuracy: 0.5534 - val_loss: 0.6793 - val_accuracy: 0.5535\n",
      "Epoch 3/15\n",
      "7803/7803 [==============================] - 24s 3ms/step - loss: 0.6776 - accuracy: 0.5555 - val_loss: 0.6776 - val_accuracy: 0.5614\n",
      "Epoch 4/15\n",
      "7803/7803 [==============================] - 25s 3ms/step - loss: 0.6759 - accuracy: 0.5588 - val_loss: 0.6776 - val_accuracy: 0.5622\n",
      "Epoch 5/15\n",
      "7803/7803 [==============================] - 46s 6ms/step - loss: 0.6741 - accuracy: 0.5619 - val_loss: 0.6775 - val_accuracy: 0.5628\n",
      "Epoch 6/15\n",
      "7803/7803 [==============================] - 39s 5ms/step - loss: 0.6729 - accuracy: 0.5633 - val_loss: 0.6774 - val_accuracy: 0.5651\n",
      "Epoch 7/15\n",
      "7803/7803 [==============================] - 39s 5ms/step - loss: 0.6721 - accuracy: 0.5656 - val_loss: 0.6760 - val_accuracy: 0.5666\n",
      "Epoch 8/15\n",
      "7803/7803 [==============================] - 31s 4ms/step - loss: 0.6713 - accuracy: 0.5661 - val_loss: 0.6766 - val_accuracy: 0.5643\n",
      "Epoch 9/15\n",
      "7803/7803 [==============================] - 40s 5ms/step - loss: 0.6703 - accuracy: 0.5685 - val_loss: 0.6773 - val_accuracy: 0.5637\n",
      "Epoch 10/15\n",
      "7803/7803 [==============================] - 41s 5ms/step - loss: 0.6696 - accuracy: 0.5699 - val_loss: 0.6759 - val_accuracy: 0.5676\n",
      "Epoch 11/15\n",
      "7803/7803 [==============================] - 38s 5ms/step - loss: 0.6686 - accuracy: 0.5715 - val_loss: 0.6740 - val_accuracy: 0.5690\n",
      "Epoch 12/15\n",
      "7803/7803 [==============================] - 39s 5ms/step - loss: 0.6683 - accuracy: 0.5723 - val_loss: 0.6735 - val_accuracy: 0.5709\n",
      "Epoch 13/15\n",
      "7803/7803 [==============================] - 40s 5ms/step - loss: 0.6673 - accuracy: 0.5735 - val_loss: 0.6753 - val_accuracy: 0.5705\n",
      "Epoch 14/15\n",
      "7803/7803 [==============================] - 40s 5ms/step - loss: 0.6667 - accuracy: 0.5744 - val_loss: 0.6727 - val_accuracy: 0.5691\n",
      "Epoch 15/15\n",
      "7803/7803 [==============================] - 40s 5ms/step - loss: 0.6662 - accuracy: 0.5755 - val_loss: 0.6720 - val_accuracy: 0.5727\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7ff1a7f18820>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "input_layer = Input((768,))\n",
    "dense = Dense(128, activation=\"relu\")(input_layer)\n",
    "output = Dense(2, activation=\"softmax\")(dense)  # Output values is the number of classes.\n",
    "rnn_model = Model(input_layer, output)\n",
    "\n",
    "# Compile the model.\n",
    "rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model. MAKE SURE TO CHANGE THIS TO 25 EPOCHS.\n",
    "rnn_model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models to try:\n",
    "### - SVM\n",
    "### - Naive Bayes\n",
    "### - kNN\n",
    "### - Random Forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to DataFrame.\n",
    "predictions = rnn_model.predict(x_test)\n",
    "predictions = np.array(list(map(lambda x: 0 if x[0] > x[1] else 1, predictions)))\n",
    "test_data[\"prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame.\n",
    "aggregated = pd.DataFrame(test_stamps, columns=[\"time\"])\n",
    "\n",
    "# Get the actual.\n",
    "agg_count = test_data.loc[:, [\"increase\"]].groupby(\"time\").count()\n",
    "agg_sum = test_data.loc[:, [\"increase\", \"prediction\"]].groupby(\"time\").sum()\n",
    "\n",
    "# Change column names.\n",
    "agg_count = agg_count.rename(columns={\"increase\": \"total_count\"})\n",
    "agg_sum = agg_sum.rename(columns={\"increase\": \"actual\", \"prediction\": \"pred_count\"})\n",
    "\n",
    "# Final join.\n",
    "agg = agg_count.join(agg_sum)\n",
    "agg[\"actual\"] = agg[\"actual\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "agg[\"pred_perc\"] = agg[\"pred_count\"] / agg[\"total_count\"]\n",
    "agg = agg[[\"actual\", \"total_count\", \"pred_count\", \"pred_perc\"]]\n",
    "agg.to_csv(\"../data/XX_XX_agg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e5788945c2802237ac72a84c772d7bec8af624c1002577fbf4f87a46c8d1d03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('scholarly_project': conda)",
   "name": "python391jvsc74a57bd0bec8622bf30164f87f5685ebe98ae91d7fd7e93d67ef4c01423e53cab45c9e9c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}