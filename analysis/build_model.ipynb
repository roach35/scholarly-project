{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Roach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed Packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Input\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Feature Engineering\n",
    "Before any modeling was performed, the necessary data was collected using two distinct platforms. The first data that was collected was Twitter data. This was done using the Twitter Developer API, as well as the `tweepy` module. Tweets containing the word \"bitcoin\" were streamed for several days. This data was written to a `.pkl` file, and saved for later feature engineering.\n",
    "\n",
    "The other data that was collected was the value of a single Bitcoin. During the same interval (plus twenty-four hours after the last Tweet was recorded) that the Twitter data was collected, the value of a Bitcoin was recorded each minute, along with the corresponding time stamp.\n",
    "\n",
    "Once the Twitter and Bitcoin data was recorded, further feature engineering was employed. For each Tweet stored, the corresponding price of Bitcoin at the time the Tweet was made was added as the `inital_price` for the Tweet. Then, for each Tweet, if the price of Bitcoin increased within three hours of the time the Tweet was made, the feature `increase` was assigned a value of `1`. Otherwise, `increase` is assigned the value of `0`.\n",
    "\n",
    "Finally, for each Tweet recorded, the text of that Tweet is cleaned and standardized. This cleaned Tweet is then BERTified, and a vector of length 384 is returned. This vector is stored as the `embedded` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data.\n",
    "data = pd.read_pickle(\"../data/3_25_training_data.pkl\")\n",
    "\n",
    "# Reset the index, convert each embedding to an array.\n",
    "data = data.reset_index(drop=True)\n",
    "data[\"embedding\"] = data[\"embedding\"].apply(lambda x: np.asarray(x))\n",
    "\n",
    "# Remove bad rows.\n",
    "max_stamps = map(lambda x: x - timedelta(hours=12), set(data[\"time\"].tolist()))\n",
    "data = data.loc[data[\"time\"].isin(list(max_stamps)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new train-test split (for aggregation).\n",
    "stamps = np.unique(data.time)\n",
    "data.set_index([\"time\"], inplace=True)\n",
    "test_stamps = np.random.choice(stamps, size=int(stamps.shape[0] * .20))\n",
    "test_data = data.loc[test_stamps, :]\n",
    "train_data = data.loc[~data.index.isin(test_stamps), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training data has been read in, the data will be quickly inspected, to show the reader the nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 187303 rows in the DataFrame.\n",
      "There are 126717 records with an increase, and 60586 with a decrease.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Investigate the DataFrame.\n",
    "print(\"There are\", len(data), \"rows in the DataFrame.\")\n",
    "print(\"There are\", len(data.loc[data[\"increase\"] == 1, ]), \"records with an increase, and\", \n",
    "        len(data.loc[data[\"increase\"] == 0, ]), \"with a decrease.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing data.\n",
    "x_train_sk = train_data[\"embedding\"]\n",
    "y_train_sk = train_data[\"increase\"]\n",
    "x_test_sk = test_data[\"embedding\"]\n",
    "y_test_sk = test_data[\"increase\"]\n",
    "\n",
    "# Conver to Tensors.\n",
    "x_train = tf.convert_to_tensor(x_train_sk.to_list())\n",
    "y_train = tf.convert_to_tensor(y_train_sk.to_list())\n",
    "x_test = tf.convert_to_tensor(x_test_sk.to_list())\n",
    "y_test = tf.convert_to_tensor(y_test_sk.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "4822/4822 [==============================] - 15s 3ms/step - loss: 0.6172 - accuracy: 0.6838 - val_loss: 0.6457 - val_accuracy: 0.6422\n",
      "Epoch 2/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.6073 - accuracy: 0.6870 - val_loss: 0.6288 - val_accuracy: 0.6473\n",
      "Epoch 3/25\n",
      "4822/4822 [==============================] - 19s 4ms/step - loss: 0.6022 - accuracy: 0.6888 - val_loss: 0.6340 - val_accuracy: 0.6479\n",
      "Epoch 4/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5981 - accuracy: 0.6900 - val_loss: 0.6261 - val_accuracy: 0.6492\n",
      "Epoch 5/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5956 - accuracy: 0.6922 - val_loss: 0.6287 - val_accuracy: 0.6489\n",
      "Epoch 6/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5937 - accuracy: 0.6938 - val_loss: 0.6352 - val_accuracy: 0.6501\n",
      "Epoch 7/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5920 - accuracy: 0.6950 - val_loss: 0.6214 - val_accuracy: 0.6622\n",
      "Epoch 8/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5902 - accuracy: 0.6964 - val_loss: 0.6188 - val_accuracy: 0.6639\n",
      "Epoch 9/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5887 - accuracy: 0.6978 - val_loss: 0.6216 - val_accuracy: 0.6571\n",
      "Epoch 10/25\n",
      "4822/4822 [==============================] - 19s 4ms/step - loss: 0.5872 - accuracy: 0.6985 - val_loss: 0.6274 - val_accuracy: 0.6584\n",
      "Epoch 11/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5863 - accuracy: 0.6993 - val_loss: 0.6287 - val_accuracy: 0.6590\n",
      "Epoch 12/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5852 - accuracy: 0.7000 - val_loss: 0.6327 - val_accuracy: 0.6560\n",
      "Epoch 13/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5841 - accuracy: 0.6997 - val_loss: 0.6262 - val_accuracy: 0.6596\n",
      "Epoch 14/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5831 - accuracy: 0.7010 - val_loss: 0.6205 - val_accuracy: 0.6638\n",
      "Epoch 15/25\n",
      "4822/4822 [==============================] - 17s 4ms/step - loss: 0.5823 - accuracy: 0.7016 - val_loss: 0.6186 - val_accuracy: 0.6628\n",
      "Epoch 16/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5812 - accuracy: 0.7018 - val_loss: 0.6226 - val_accuracy: 0.6643\n",
      "Epoch 17/25\n",
      "4822/4822 [==============================] - 19s 4ms/step - loss: 0.5803 - accuracy: 0.7015 - val_loss: 0.6277 - val_accuracy: 0.6606\n",
      "Epoch 18/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5793 - accuracy: 0.7026 - val_loss: 0.6443 - val_accuracy: 0.6574\n",
      "Epoch 19/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5788 - accuracy: 0.7030 - val_loss: 0.6229 - val_accuracy: 0.6656\n",
      "Epoch 20/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5783 - accuracy: 0.7030 - val_loss: 0.6214 - val_accuracy: 0.6657\n",
      "Epoch 21/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5773 - accuracy: 0.7036 - val_loss: 0.6196 - val_accuracy: 0.6649\n",
      "Epoch 22/25\n",
      "4822/4822 [==============================] - 19s 4ms/step - loss: 0.5767 - accuracy: 0.7050 - val_loss: 0.6276 - val_accuracy: 0.6633\n",
      "Epoch 23/25\n",
      "4822/4822 [==============================] - 18s 4ms/step - loss: 0.5764 - accuracy: 0.7042 - val_loss: 0.6201 - val_accuracy: 0.6635\n",
      "Epoch 24/25\n",
      "4822/4822 [==============================] - 19s 4ms/step - loss: 0.5757 - accuracy: 0.7038 - val_loss: 0.6282 - val_accuracy: 0.6631\n",
      "Epoch 25/25\n",
      "4822/4822 [==============================] - 19s 4ms/step - loss: 0.5752 - accuracy: 0.7044 - val_loss: 0.6175 - val_accuracy: 0.6661\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x7fc5776110d0>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "input_layer = Input((768,))\n",
    "dense = Dense(128, activation=\"relu\")(input_layer)\n",
    "output = Dense(2, activation=\"softmax\")(dense)  # Output values is the number of classes.\n",
    "rnn_model = Model(input_layer, output)\n",
    "\n",
    "# Compile the model.\n",
    "rnn_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model. MAKE SURE TO CHANGE THIS TO 25 EPOCHS.\n",
    "rnn_model.fit(x_train, y_train, epochs=25, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models to try:\n",
    "### - SVM\n",
    "### - Naive Bayes\n",
    "### - kNN\n",
    "### - Random Forrests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to DataFrame.\n",
    "predictions = rnn_model.predict(x_test)\n",
    "predictions = np.array(list(map(lambda x: 0 if x[0] > x[1] else 1, predictions)))\n",
    "test_data[\"prediction\"] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame.\n",
    "aggregated = pd.DataFrame(test_stamps, columns=[\"time\"])\n",
    "\n",
    "# Get the actual.\n",
    "agg_count = test_data.loc[:, [\"increase\"]].groupby(\"time\").count()\n",
    "agg_sum = test_data.loc[:, [\"increase\", \"prediction\"]].groupby(\"time\").sum()\n",
    "\n",
    "# Change column names.\n",
    "agg_count = agg_count.rename(columns={\"increase\": \"total_count\"})\n",
    "agg_sum = agg_sum.rename(columns={\"increase\": \"actual\", \"prediction\": \"pred_count\"})\n",
    "\n",
    "# Final join.\n",
    "agg = agg_count.join(agg_sum)\n",
    "agg[\"actual\"] = agg[\"actual\"].apply(lambda x: 0 if x == 0 else 1)\n",
    "agg[\"pred_perc\"] = agg[\"pred_count\"] / agg[\"total_count\"]\n",
    "agg = agg[[\"actual\", \"total_count\", \"pred_count\", \"pred_perc\"]]\n",
    "agg.to_csv(\"../data/3_25_agg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4e5788945c2802237ac72a84c772d7bec8af624c1002577fbf4f87a46c8d1d03"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit ('scholarly_project': conda)",
   "name": "python391jvsc74a57bd0bec8622bf30164f87f5685ebe98ae91d7fd7e93d67ef4c01423e53cab45c9e9c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}