{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacob Roach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed Packages.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection and Feature Engineering\n",
    "Before any modeling was performed, the necessary data was collected using two distinct platforms. The first data that was collected was Twitter data. This was done using the Twitter Developer API, as well as the `tweepy` module. Tweets containing the word \"bitcoin\" were streamed for several days. This data was written to a `.pkl` file, and saved for later feature engineering.\n",
    "\n",
    "The other data that was collected was the value of a single Bitcoin. During the same interval (plus twenty-four hours after the last Tweet was recorded) that the Twitter data was collected, the value of a Bitcoin was recorded each minute, along with the corresponding time stamp.\n",
    "\n",
    "Once the Twitter and Bitcoin data was recorded, further feature engineering was employed. For each Tweet stored, the corresponding price of Bitcoin at the time the Tweet was made was added as the `inital_price` for the Tweet. Then, for each Tweet, if the price of Bitcoin increased within twenty-four hours of the time the Tweet was made, the feature `increase` was assigned a value of `1`. Otherwise, `increase` is assigned the value of `0`.\n",
    "\n",
    "Finally, for each Tweet recorded, the text of that Tweet is cleaned and standardized. This cleaned Tweet is then BERTified, and a vector of length 512 is returned. This vector is stored as the `bertified` feature. Only the `bertified` and `increase` features are kept, and these form the training data to be used in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the training data.\n",
    "data = pd.read_pickle(\"../data/training_data.pkl\")\n",
    "\n",
    "# Reset the index.\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Convert each list to an array.\n",
    "data[\"bertified\"] = data[\"bertified\"].apply(lambda x: np.asarray(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training data has been read in, the data will be quickly inspected, to show the reader the nature of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 114233 rows in the DataFrame.\n",
      "There are 55912 records with an increase, and 58321 with a decrease.\n",
      "\n",
      "count    114233.000000\n",
      "mean          0.489456\n",
      "std           0.499891\n",
      "min           0.000000\n",
      "25%           0.000000\n",
      "50%           0.000000\n",
      "75%           1.000000\n",
      "max           1.000000\n",
      "Name: increase, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Investigate the DataFrame.\n",
    "print(\"There are\", len(data), \"rows in the DataFrame.\")\n",
    "print(\"There are\", len(data.loc[data[\"increase\"] == 1, ]), \"records with an increase, and\", len(data.loc[data[\"increase\"] == 0, ]), \"with a decrease.\\n\")\n",
    "\n",
    "# Now, show the summary of the data.\n",
    "print(data[\"increase\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training data has been loaded, it can be partitioned into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and testing.\n",
    "x_train, x_test, y_train, y_test = train_test_split(data[\"bertified\"], data[\"increase\"], test_size=.10)\n",
    "\n",
    "# Conver to Tensors.\n",
    "x_train = tf.convert_to_tensor(x_train.to_list())\n",
    "y_train = tf.convert_to_tensor(y_train.to_list())\n",
    "x_test = tf.convert_to_tensor(x_test.to_list())\n",
    "y_test = tf.convert_to_tensor(y_test.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "3213/3213 [==============================] - 9s 3ms/step - loss: 0.6793 - accuracy: 0.5453 - val_loss: 0.6610 - val_accuracy: 0.5728\n",
      "Epoch 2/30\n",
      "3213/3213 [==============================] - 7s 2ms/step - loss: 0.6471 - accuracy: 0.5998 - val_loss: 0.6512 - val_accuracy: 0.5834\n",
      "Epoch 3/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.6261 - accuracy: 0.6284 - val_loss: 0.6463 - val_accuracy: 0.5899\n",
      "Epoch 4/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.6040 - accuracy: 0.6536 - val_loss: 0.6452 - val_accuracy: 0.5938\n",
      "Epoch 5/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.5889 - accuracy: 0.6712 - val_loss: 0.6444 - val_accuracy: 0.6010\n",
      "Epoch 6/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.5714 - accuracy: 0.6866 - val_loss: 0.6487 - val_accuracy: 0.6032\n",
      "Epoch 7/30\n",
      "3213/3213 [==============================] - 7s 2ms/step - loss: 0.5539 - accuracy: 0.6992 - val_loss: 0.6563 - val_accuracy: 0.6023\n",
      "Epoch 8/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.5425 - accuracy: 0.7104 - val_loss: 0.6707 - val_accuracy: 0.5986\n",
      "Epoch 9/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.5267 - accuracy: 0.7237 - val_loss: 0.6683 - val_accuracy: 0.6008\n",
      "Epoch 10/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.5150 - accuracy: 0.7333 - val_loss: 0.6853 - val_accuracy: 0.5980\n",
      "Epoch 11/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.5027 - accuracy: 0.7412 - val_loss: 0.6944 - val_accuracy: 0.6084\n",
      "Epoch 12/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.4920 - accuracy: 0.7461 - val_loss: 0.7026 - val_accuracy: 0.6016\n",
      "Epoch 13/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4829 - accuracy: 0.7540 - val_loss: 0.7055 - val_accuracy: 0.6057\n",
      "Epoch 14/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4709 - accuracy: 0.7627 - val_loss: 0.7232 - val_accuracy: 0.6036\n",
      "Epoch 15/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4640 - accuracy: 0.7657 - val_loss: 0.7318 - val_accuracy: 0.5987\n",
      "Epoch 16/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.4565 - accuracy: 0.7726 - val_loss: 0.7453 - val_accuracy: 0.6057\n",
      "Epoch 17/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4449 - accuracy: 0.7772 - val_loss: 0.7534 - val_accuracy: 0.6050\n",
      "Epoch 18/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4429 - accuracy: 0.7798 - val_loss: 0.7635 - val_accuracy: 0.6026\n",
      "Epoch 19/30\n",
      "3213/3213 [==============================] - 8s 2ms/step - loss: 0.4363 - accuracy: 0.7846 - val_loss: 0.7768 - val_accuracy: 0.6082\n",
      "Epoch 20/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4255 - accuracy: 0.7907 - val_loss: 0.7868 - val_accuracy: 0.6060\n",
      "Epoch 21/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4203 - accuracy: 0.7921 - val_loss: 0.7890 - val_accuracy: 0.6019\n",
      "Epoch 22/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4134 - accuracy: 0.7980 - val_loss: 0.8057 - val_accuracy: 0.6046\n",
      "Epoch 23/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4086 - accuracy: 0.7982 - val_loss: 0.8280 - val_accuracy: 0.6077\n",
      "Epoch 24/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.4011 - accuracy: 0.8050 - val_loss: 0.8331 - val_accuracy: 0.6084\n",
      "Epoch 25/30\n",
      "3213/3213 [==============================] - 9s 3ms/step - loss: 0.3977 - accuracy: 0.8040 - val_loss: 0.8345 - val_accuracy: 0.6099\n",
      "Epoch 26/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.3890 - accuracy: 0.8111 - val_loss: 0.8528 - val_accuracy: 0.6107\n",
      "Epoch 27/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.3865 - accuracy: 0.8111 - val_loss: 0.8563 - val_accuracy: 0.6109\n",
      "Epoch 28/30\n",
      "3213/3213 [==============================] - 9s 3ms/step - loss: 0.3830 - accuracy: 0.8155 - val_loss: 0.8752 - val_accuracy: 0.6077\n",
      "Epoch 29/30\n",
      "3213/3213 [==============================] - 8s 3ms/step - loss: 0.3777 - accuracy: 0.8163 - val_loss: 0.8807 - val_accuracy: 0.6077\n",
      "Epoch 30/30\n",
      "3213/3213 [==============================] - 9s 3ms/step - loss: 0.3738 - accuracy: 0.8189 - val_loss: 0.8921 - val_accuracy: 0.6069\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1474c27c0>"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model.\n",
    "input_layer = Input((512,))\n",
    "dense = Dense(128, activation=\"relu\")(input_layer)\n",
    "output = Dense(2, activation=\"softmax\")(dense)  # Output values is the number of classes.\n",
    "model = Model(input_layer, output)\n",
    "\n",
    "# Compile the model.\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", \n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model.\n",
    "model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "357/357 [==============================] - 1s 4ms/step - loss: 1.1049 - accuracy: 0.6046\n"
     ]
    },
    {
     "data": {
      "text/plain": "[1.1049458980560303, 0.6046043634414673]"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the model.\n",
    "model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python382jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}